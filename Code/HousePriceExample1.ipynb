{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# House Price Prediction\n",
        "\n",
        "Let's look at supervised learning in a classic ML course example: predicting house prices.\n",
        "\n",
        "Our goal is going to build a prediction model to predict the sale price of a house based on easily observed features. The data are from the Seattle, WA area for a few months in 2014.\n",
        "\n"
      ],
      "metadata": {
        "id": "qQYHn_ZA43WQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by importing packages we will be using. See [our first example](https://colab.research.google.com/github/chansen776/MBA-ML-Course-Materials/blob/main/Code/BiasVarianceExample1.ipynb).\n"
      ],
      "metadata": {
        "id": "hVwPwD_75j3o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUO7PPC342qq"
      },
      "outputs": [],
      "source": [
        "# Install extra libraries\n",
        "!pip install formulaic\n",
        "from formulaic import model_matrix\n",
        "\n",
        "# Import relevant packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn.model_selection import cross_validate, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "\n",
        "# Seed we will use for random number generators\n",
        "rng = 713\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and examine the data\n",
        "\n",
        "We need to load the data. We will do this directly from a github repository for the course."
      ],
      "metadata": {
        "id": "HGTjeNS_5r_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"https://raw.githubusercontent.com/chansen776/MBA-ML-Course-Materials/main/Data/WAHousePrice.xlsx\"\n",
        "data = pd.read_excel(file)\n",
        "data.shape  # See size of dataset"
      ],
      "metadata": {
        "id": "HZ7XYfNl5wIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what's in the data"
      ],
      "metadata": {
        "id": "Uawq8W9i6NF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "WOd_qOmg6Nwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "NNBQ2oc66VdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The maximum price 2.659000e+07 = 26,590,000 seems very high. We'll look at this again in a second. There are also some very low prices and properties with 0 bedrooms and bathrooms. Nothing else in the summary statistics stands out as particularly noteworthy."
      ],
      "metadata": {
        "id": "ATh6tWpfX7lY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly examine the 0 bedroom and 0 bathroom observations."
      ],
      "metadata": {
        "id": "DfMcCMY1SFR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's find the observations with 0 bedrooms or 0 bathrooms and see what they look like\n",
        "data[(data['bedrooms'] == 0) | (data['bathrooms'] == 0)]"
      ],
      "metadata": {
        "id": "KKexVSP_Ss25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm pretty confident that houses with 3000+ square feet of living space should have bathrooms and bedrooms. We could try to *impute* what seem to be the missing values of bathrooms and bedrooms for these two observations. Because it's only two observations, I'm just going to delete them instead."
      ],
      "metadata": {
        "id": "u5QrRPhGTTb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the flagged observations\n",
        "data = data.drop(data[(data['bedrooms'] == 0) | (data['bathrooms'] == 0)].index)"
      ],
      "metadata": {
        "id": "RwOtqUt9UBRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "id": "tO6mB_Do6mku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the variables `street`, `city`, `statezip`, and `country` are not numeric, so don't show up in the summary table. We'll look at these variables after examining the outcome in a bit more detail."
      ],
      "metadata": {
        "id": "vVRInON3N63g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outcome variable**\n",
        "\n",
        "Let's start by seeing what the outcome variable looks like."
      ],
      "metadata": {
        "id": "Kw4BOYN3DS25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Smoothed histogram of the outcome variable, price\n",
        "sns.displot(data=data, x='price', kind='kde')\n",
        "plt.title('Smoothed histogram of Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JdPkwUZbDvZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(data['sqft_living'], data['price'])\n",
        "plt.xlabel('sqft_living')\n",
        "plt.ylabel('price')\n",
        "plt.title('Scatter plot of Price vs Square Feet')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yog_xRoeYVAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two observations that are probably worth another look. Also recall that there was at least one very low price from the summary statistics. Let's look at both."
      ],
      "metadata": {
        "id": "hwjd2j9rYlmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's find the observations with very low prices and see what they look like\n",
        "data[data['price'] < 50000]"
      ],
      "metadata": {
        "id": "zb4dHNQsSLeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems a bit out of the realm of possibility to have a $7800 dollar property in the Seattle area in 2014, though the property seems not particularly desirable. I'm going to assume that the price here is incorrect."
      ],
      "metadata": {
        "id": "jEPgRHDNSaw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's find the observations with very high prices and see what they look like\n",
        "data[data['price'] > 5000000]\n"
      ],
      "metadata": {
        "id": "Lj546lWFYqW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can believe that a 10000 square foot property sold for $7M.\n",
        "\n",
        "I have a hard time believing a 2 bathroom, 3 bedroom, 1180 square foot property sold for ~\\$27M. This seems like a mistake. I'm also having a hard time believing that a 2.5 bathroom, 3 bedroom, 2190 square foot property sold for ~$13M.\n",
        "\n",
        "I am going to treat both of these observations as \"mistakes\" and drop them from my data. I am going to recognize that if these are real observations, there is a small chance I will see future properties that I make GIGANTIC mistakes on.\n",
        "\n",
        "We can/should also run the whole thing with the dropped observations included to gauge robustness.\n",
        "\n",
        "Finally, we could just use an evaluation metric that is robust to a couple outliers like the **median absolute deviation**."
      ],
      "metadata": {
        "id": "pU877AU7ZPUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the flagged observations\n",
        "data = data.drop(data[data['price'] < 50000].index)\n",
        "data = data.drop(data[data['price'] > 10000000].index)"
      ],
      "metadata": {
        "id": "vyHGl2UKazJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our price variable is highly skewed. It is very commeon to see data-scientists build models a right-skewed variable that only takes on positive values by taking a `log` transformation first. I.e. instead by building a model for `price`, we might instead build a model for `log(price)`.\n",
        "\n",
        "**Remember that our goal is to predict `price` though!** Make sure you validate on the basis of what you care about."
      ],
      "metadata": {
        "id": "N9gHuBoOEOPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new column with the log of price\n",
        "data['log_price'] = np.log(data['price'])\n",
        "\n",
        "# Smoothed histogram of log price\n",
        "sns.displot(data=data, x='log_price', kind='kde')\n",
        "plt.title('Smoothed histogram of log(Price)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T9kQ7vusENjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(data['sqft_living'], data['log_price'])\n",
        "plt.xlabel('sqft_living')\n",
        "plt.ylabel('price')\n",
        "plt.title('Scatter plot of log(Price) vs Square Feet')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "01Z8MJOSUicp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorical Variables**\n",
        "\n",
        "The four variables of type \"object\" are not numeric but coded as text. Let's look at what they are. In the following code blocks, we are just going to tabulate each of our categorical variables by looking at the `value_counts` property of our data."
      ],
      "metadata": {
        "id": "2nhLnWdnC15E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['street'].value_counts()"
      ],
      "metadata": {
        "id": "yIHslh748P1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most street addresses have only one property. Without better geographic knowledge, it's going to be hard to use this variable.\n",
        "\n",
        "**Question:** If you really cared, how could you use the street address to get something potentially more useful? (Think about, e.g., google street view, google maps and routing, ...)"
      ],
      "metadata": {
        "id": "SA8BYDoC8Z2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['city'].value_counts()"
      ],
      "metadata": {
        "id": "HCTrdnLG7RN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "City might be useful, but it is very unbalanced.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Do we think the actual name of the city matters? Why might the variable `city` contain useful information?\n",
        "\n",
        "2. If we want to predict something outside of the Seattle area, is the variable `city` useful? Could we make it useful with some more effort?"
      ],
      "metadata": {
        "id": "Wc0vXdpQ9GP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['statezip'].value_counts()"
      ],
      "metadata": {
        "id": "TrDAoWmE73aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zip code (`statezip`) seems like it might have more *interesting* variation than `city`.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Do we think the actual numeric value of a property's zip code matters? Why might the variable `statezip` contain useful information?\n",
        "\n",
        "2. If we want to predict something outside of the Seattle area, is the variable `statezip` useful? Could we make it useful with some more effort?"
      ],
      "metadata": {
        "id": "CBSDYNmx9Peg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['country'].value_counts()"
      ],
      "metadata": {
        "id": "l9DgmDtJ8DeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can't use the variable `country` - It doesn't *vary* ðŸ™‚."
      ],
      "metadata": {
        "id": "8BWgtK6z8JZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aside: Categorical Variables.**\n",
        "\n",
        "The typical way to use (unordered) categorical variables is to encode them as dummy variables/binary variables/use one-hot-encoding --  all of which are jargon for making a new set of variables that are 0 or 1 with 1 indicating the observation belongs to a category and 0 indicates it does not.\n",
        "\n",
        "For example, if we had a variable `color` with two categories \"red\" and \"blue\", we just create two new variables `red` and `blue` where variable `red` = 1 for all observations that are red (and 0 otherwise) and `blue` is defined similarly. (Note: For standard linear models, you typically exclude one of the dummy variables. In our toy example, we don't need both the variables `red` and `blue` because if `red` = 1 we know `blue` = 0 and viceversa. That is, they have the same information.)\n",
        "\n",
        "We'll look at doing this in our price example when we consider including `city` and `statezip` in our model."
      ],
      "metadata": {
        "id": "lzakdBbUb0ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before doing anything else, let's drop the variables we definitely won't use."
      ],
      "metadata": {
        "id": "fKBUm9pS_a--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns=['date','street','country'])"
      ],
      "metadata": {
        "id": "khWiCZII_e84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we also see that we have many other variables that are effectively capturing qualitative information or a mix. E.g. `bathrooms`, `bedrooms`, `floors`, `waterfront`, `view`, and `condition` might be thought of as categorical.\n",
        "\n",
        "Let's look at these variables more carefully."
      ],
      "metadata": {
        "id": "kBggP2xO_6cA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['bathrooms'].value_counts())\n",
        "print('\\n')\n",
        "print(data['bedrooms'].value_counts())\n",
        "print('\\n')\n",
        "print(data['floors'].value_counts())\n",
        "print('\\n')\n",
        "print(data['waterfront'].value_counts())\n",
        "print('\\n')\n",
        "print(data['view'].value_counts())\n",
        "print('\\n')\n",
        "print(data['condition'].value_counts())"
      ],
      "metadata": {
        "id": "FxcmTUmJBsKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly, these variables all have an order to them. `bathrooms`, `bedrooms`, and `floors` also (arguably) have cardinal value. `view` and `condition` are clearly qualitative, but do have ordinal meaning. `waterfront` is already a dummy variable.\n",
        "\n",
        "We could treat these by including them as dummy variables, but it also makes sense to use them as is (which is what we are going to do). Many ML procedures will happily and appropriately deal with ordered categorical features and numeric features with few values."
      ],
      "metadata": {
        "id": "TAJI0_7yy0Rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mixed variables**\n",
        "\n",
        "The variables `sqft_basement` and `yr_renovated` have a 0 category indicating \"no basement\" or \"never renovated\" respectively.\n",
        "\n",
        "We can add variables indicating the qualitative information. For many learners, adding this variable is also unnecessary."
      ],
      "metadata": {
        "id": "pNO8PYJXvmj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a variable for being renovated\n",
        "data['renovated_flag'] = np.where(data['yr_renovated'] == 0, 0, 1)\n",
        "\n",
        "# Add a variable for having a basement\n",
        "data['basement_flag'] = np.where(data['sqft_basement'] == 0, 0, 1)"
      ],
      "metadata": {
        "id": "oqEo4UkTvssh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the the square footage variables `sqft_living`, `sqft_above` and `sqft_basement` capture different information?"
      ],
      "metadata": {
        "id": "20EfbUuMfhJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's calculate the correlation of sqft_living with sqft_above+sqft_basement\n",
        "print('Correlation of sqft_living with (sqft_above+sqft_basement):',\n",
        "      data['sqft_living'].corr(data['sqft_above']+data['sqft_basement']))\n",
        "\n",
        "# Let's calculate the correlation with each element instead\n",
        "print('Correlation of sqft_living with sqft_above:',\n",
        "      data['sqft_living'].corr(data['sqft_above']))\n",
        "print('Correlation of sqft_living with sqft_basement:',\n",
        "      data['sqft_living'].corr(data['sqft_basement']))\n"
      ],
      "metadata": {
        "id": "oUZ3VAEsfqI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cannot use `sqft_living` and both `sqft_basement` and `sqft_above` in the same linear model estimated by least squares (or least absolute values).\n",
        "\n",
        "**Question:** How do we choose which to use?"
      ],
      "metadata": {
        "id": "KxD3-Zz5hdca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation\n",
        "\n",
        "We are going to use 5-fold CV to evaluate our models. We want to keep the cross-validation folds the same across all of the models we build, so we're going to predefine the splits."
      ],
      "metadata": {
        "id": "gO-cm2jqWdH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cvsplit = KFold(n_splits=5, shuffle=True, random_state=rng)"
      ],
      "metadata": {
        "id": "FCFKawjSMkvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to choose how to evaluate the models. For illustration, we're going to consider performance based on both MAE and MSE. We are going to estimate models using both `price` and `log_price` for illustration.\n",
        "\n",
        "When we build a model for `log_price`, we don't directly build a prediction rule for `price`. We are going to use the simplest approach to turning our `log_price` prediction into a `price` prediction by exponentiating. I.e. if $\\widehat{\\texttt{log_price}}$ is our prediction for `log_price`, we obtain a prediction for `price` as $\\exp\\{\\widehat{\\texttt{log_price}}\\}$.\n",
        "\n",
        "We're going to define functions to produce MSE and MAE from `log_price` predictions."
      ],
      "metadata": {
        "id": "d-NHEZ36W5SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expmse(y_true, y_pred):\n",
        "\n",
        "  y_true = y_true.to_numpy()\n",
        "  y_pred[y_pred < -20] = -20\n",
        "  y_pred[y_pred > 20] = 20 # Prevent overflow issues in really bad models because we are going to exponentiate\n",
        "  negmse = -np.mean((np.exp(y_true) - np.exp(y_pred))**2)\n",
        "\n",
        "  return negmse\n",
        "\n",
        "# Create a scorer object using the expmse function\n",
        "expmse_score = make_scorer(expmse)\n",
        "\n",
        "def expmae(y_true, y_pred):\n",
        "\n",
        "  y_true = y_true.to_numpy()\n",
        "  y_pred[y_pred < -20] = -20\n",
        "  y_pred[y_pred > 20] = 20 # Prevent overflow issues in really bad models because we are going to exponentiate\n",
        "  negmae = -np.mean(np.abs(np.exp(y_true) - np.exp(y_pred)))\n",
        "\n",
        "  return negmae\n",
        "\n",
        "# Create a scorer object using the expmae function\n",
        "expmae_score = make_scorer(expmae)\n"
      ],
      "metadata": {
        "id": "Htclo5CJNsXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline start\n",
        "\n",
        "As simple benchmark, let's look at sample means and medians.\n",
        "\n",
        "We're going to define functions for using the mean and median as prediction rules that we can use with `sklearn`'s built in cross-validation functions."
      ],
      "metadata": {
        "id": "thTV0AIN_Wop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to use mean as estimator and make prediction\n",
        "class MeanEstimator(BaseEstimator, RegressorMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        # Compute the mean of y during fitting\n",
        "        self.mean_ = np.mean(y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Return the mean for all predictions\n",
        "        return np.full(len(X), self.mean_)\n",
        "\n",
        "# Define function to use median as estimator and make prediction\n",
        "class MedianEstimator(BaseEstimator, RegressorMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        # Compute the mean of y during fitting\n",
        "        self.median_ = np.median(y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Return the mean for all predictions\n",
        "        return np.full(len(X), self.median_)\n"
      ],
      "metadata": {
        "id": "DF3ZbJg8Pq8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at how well the sample mean and sample median of `price` do for predicting `price` by 5-fold cross-validation."
      ],
      "metadata": {
        "id": "eRvHRko8YKNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation RMSE and MAE using sample mean of price as prediction rule\n",
        "# The \"scoring\" argument tells cross-validation what out-of-sample prediction\n",
        "# metrics to compute.\n",
        "levmean_mse = cross_validate(MeanEstimator(), data['price'], data['price'],\n",
        "                             scoring=('neg_mean_squared_error','neg_mean_absolute_error'), cv=cvsplit)\n",
        "print('Level Mean: CV RMSE: {m1:=.2f}; CV MAE: {m2:=.2f}'\n",
        "  .format(m1=np.sqrt(-levmean_mse['test_neg_mean_squared_error'].mean()),\n",
        "          m2=-levmean_mse['test_neg_mean_absolute_error'].mean()))\n",
        "\n",
        "# Cross validation RMSE and MAE using sample median of price as prediction rule\n",
        "# The \"scoring\" argument tells cross-validation what out-of-sample prediction\n",
        "# metrics to compute.\n",
        "levmedian_mse = cross_validate(MedianEstimator(), data['price'], data['price'],\n",
        "                               scoring=('neg_mean_squared_error','neg_mean_absolute_error'), cv=cvsplit)\n",
        "print('Level Median: CV RMSE: {m1:=.2f}; CV MAE: {m2:=.2f}'\n",
        "  .format(m1=np.sqrt(-levmedian_mse['test_neg_mean_squared_error'].mean()),\n",
        "          m2=-levmedian_mse['test_neg_mean_absolute_error'].mean()))\n",
        "\n",
        "# We are going to use cross-validated MSE from the mean as our benchmark for MSE\n",
        "# We are going to use cross-validated MAE from the median as our benchmark for MAE\n",
        "benchMSE = -levmean_mse['test_neg_mean_squared_error'].mean()\n",
        "benchMAE = -levmedian_mse['test_neg_mean_absolute_error'].mean()"
      ],
      "metadata": {
        "id": "Y78A3ulaUXhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a table to keep track of results\n",
        "model_results = pd.DataFrame()\n",
        "model_results['Model'] = ['Mean', 'Median']\n",
        "model_results['RMSE'] = [np.sqrt(-levmean_mse['test_neg_mean_squared_error'].mean()),\n",
        "                         np.sqrt(-levmedian_mse['test_neg_mean_squared_error'].mean())]\n",
        "model_results['R2 - MSE'] = [None,None]\n",
        "model_results['MAE'] = [-levmean_mse['test_neg_mean_absolute_error'].mean(),\n",
        "                        -levmedian_mse['test_neg_mean_absolute_error'].mean()]\n",
        "model_results['R2 - MAE'] = [None,None]\n",
        "model_results['p'] = [None,None]\n",
        "model_results['p_use'] = [None,None]\n",
        "model_results"
      ],
      "metadata": {
        "id": "Cyy4YO2jFtB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline linear models.\n",
        "\n",
        "## Think about the model\n",
        "\n",
        "For black box machine learners that discover nonlinearity automatically, thinking about how to model nonlinearity and other transformations to make of our available variables is not a big issue. The ability to put less time into  thinking about these issues is a major plus for ML algorithms. **You should always be thoughtful about variables in a model though.**\n",
        "\n",
        "For learners like linear regression, thinking about the appropriate way to include variables is potentially very important though.\n"
      ],
      "metadata": {
        "id": "SwJfKKof0ZGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nonlinearity**\n",
        "\n",
        "We should think about potential *nonlinearities* that we would like to allow our models to capture. For example, do we think that each unit increase in house's square footage at fixed values of other variables should be associated with the same change in predicted (log) price regardless of the square footage? E.g. do we think going from 1000 to 1100 square feet is the same \"value\" as going from 3000-3100 square feet? Do we think the \"value\" of going from 2000-2100 square feet is the same if we are talking about a three versus five bedroom property?\n",
        "\n",
        "A very simple and interpretable way to allow for nonlinearities is to use *polynomials* and *interactions*. E.g. we might include `sqft_living` and `sqft_living`$^2$ to allow for decreasing or increasing \"returns\" to square footage in our model, so our prediction rule would be\n",
        "\n",
        "$$\\widehat{\\texttt{price}} = b_0 + b_1 \\texttt{sqft_living} + b_2 \\texttt{sqft_living}^2 + ...$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\\widehat{\\texttt{log_price}} = b_0 + b_1 \\texttt{sqft_living} + b_2 \\texttt{sqft_living}^2 + ...$$\n",
        "\n",
        "[Note that you cannot interpret $b_1$ and $b_2$ in isolation.]\n",
        "\n",
        "We might also include the *interaction* `sqft_living`*`bedrooms` to allow the predicted price change for a given change in `sqft_living` to be different for properties with different numbers of bedrooms. In this case, our model would be\n",
        "\n",
        "$$ \\widehat{\\texttt{price}} = b_0 + b_1 \\texttt{sqft_living} + b_2 \\texttt{sqft_living}^2 + b_3\\texttt{bedrooms} + b_4 \\texttt{sqft_living}*\\texttt{bedrooms} + ...$$\n",
        "\n",
        "or\n",
        "\n",
        "$$ \\widehat{\\texttt{log_price}} = b_0 + b_1 \\texttt{sqft_living} + b_2 \\texttt{sqft_living}^2 + b_3\\texttt{bedrooms} + b_4 \\texttt{sqft_living}*\\texttt{bedrooms} + ...$$\n",
        "\n",
        "[Note that you cannot interpret $b_1$, $b_2$, $b_3$, and $b_4$ in isolation.]\n",
        "\n",
        "In our baseline prediction rules we will include squares of all \"numeric\" variable and include the interaction of `sqft_living` with `bedrooms` and with `bathrooms`"
      ],
      "metadata": {
        "id": "2NUsv_1Bugfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline models: OLS"
      ],
      "metadata": {
        "id": "_muYOXDf4eMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we're using the \"formula\" interface we imported from formulaic\n",
        "# Specify model as formula\n",
        "# Putting a variable inside \"C\" will make the variable be treated as categorical\n",
        "# by turning it into dummy variables\n",
        "# Putting a variable inside \"poly\" will a create a polynomial in that variable\n",
        "# Putting v1:v2 creates the interaction between variables v1 and v2\n",
        "base = (\"price ~ poly(bathrooms, degree=2, raw=True) + poly(bedrooms, degree=2, raw=True)\"\n",
        "             \" + poly(sqft_living, degree=2, raw=True) + poly(sqft_lot, degree=2, raw=True) + poly(floors, degree=2, raw=True) \"\n",
        "            \"+ waterfront + poly(view, degree=2, raw=True) + poly(condition, degree=2, raw=True) + \"\n",
        "            \"poly(yr_built, degree=2, raw=True) + poly(yr_renovated, degree=2, raw=True) + C(renovated_flag) + C(basement_flag)\"\n",
        "            \" + sqft_living:(bedrooms+bathrooms)\")\n",
        "\n",
        "# Create variables based on model defined in formula\n",
        "y, X = model_matrix(base, data)\n",
        "n, p_base = X.shape\n",
        "\n",
        "# Cross validation.\n",
        "baselm = cross_validate(LinearRegression(), X, y,\n",
        "                       scoring = ('neg_mean_squared_error','neg_mean_absolute_error'), cv=cvsplit)\n",
        "print('Baseline LM: CV RMSE: {m1:=.2f}; CV R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=np.sqrt(-baselm['test_neg_mean_squared_error'].mean()),\n",
        "          m2=1-(-baselm['test_neg_mean_squared_error'].mean()/benchMSE),\n",
        "          m3=p_base))\n",
        "\n",
        "print('Baseline LM: CV MAE: {m1:=.2f}; CV MAE R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=-baselm['test_neg_mean_absolute_error'].mean(),\n",
        "          m2=1-(-baselm['test_neg_mean_absolute_error'].mean()/benchMAE),\n",
        "          m3=p_base))\n",
        "\n",
        "#####################################################################\n",
        "# Model for log-price\n",
        "logy = data['log_price']\n",
        "\n",
        "# Set things up for custom scoring when using log(price)\n",
        "scoring = {'expmse': expmse_score,\n",
        "           'expmae': expmae_score}\n",
        "\n",
        "# Cross validation.\n",
        "baseloglm = cross_validate(LinearRegression(), X, logy,\n",
        "                       scoring = scoring, cv=cvsplit)\n",
        "print('Baseline logLM: CV RMSE: {m1:=.2f}; CV R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=np.sqrt(-baseloglm['test_expmse'].mean()),\n",
        "          m2=1-(-baseloglm['test_expmse'].mean()/benchMSE),\n",
        "          m3=p_base))\n",
        "\n",
        "print('Baseline logLM: CV MAE: {m1:=.2f}; CV MAE R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=-baseloglm['test_expmae'].mean(),\n",
        "          m2=1-(-baseloglm['test_expmae'].mean()/benchMAE),\n",
        "          m3=p_base))\n"
      ],
      "metadata": {
        "id": "6a9bae96b8Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add these results to our result table\n",
        "model_results = model_results._append({'Model': 'Baseline LM',\n",
        "                                       'RMSE': np.sqrt(-baselm['test_neg_mean_squared_error'].mean()),\n",
        "                                       'R2 - MSE': 1-(-baselm['test_neg_mean_squared_error'].mean()/benchMSE),\n",
        "                                       'MAE': -baselm['test_neg_mean_absolute_error'].mean(),\n",
        "                                       'R2 - MAE': 1-(-baselm['test_neg_mean_absolute_error'].mean()/benchMAE),\n",
        "                                       'p': p_base,\n",
        "                                       'p_use': p_base}, ignore_index=True)\n",
        "\n",
        "model_results = model_results._append({'Model': 'Baseline logLM',\n",
        "                                       'RMSE': np.sqrt(-baseloglm['test_expmse'].mean()),\n",
        "                                       'R2 - MSE': 1-(-baseloglm['test_expmse'].mean()/benchMSE),\n",
        "                                       'MAE': -baseloglm['test_expmae'].mean(),\n",
        "                                       'R2 - MAE': 1-(-baseloglm['test_expmae'].mean()/benchMAE),\n",
        "                                       'p': p_base,\n",
        "                                       'p_use': p_base}, ignore_index=True)\n",
        "\n",
        "model_results\n"
      ],
      "metadata": {
        "id": "TmvNLUnldWfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad. Both models are a sizeable improvement relative to the simple sample mean or median.\n",
        "\n",
        "Let's look at the `log_price` model refit using the entire dataset in more detail."
      ],
      "metadata": {
        "id": "otNhg3Vtd5Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.OLS(logy, X)\n",
        "result = model.fit(cov_type='HC3')  # cov_type = 'HC3' computes specification robust standard errors\n",
        "print(result.summary())\n",
        "\n",
        "print('\\n')\n",
        "pred_err = result.resid\n",
        "print(\"RMSE of model: \", np.sqrt(np.mean(pred_err**2)))\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "GVetOXJ5d4Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "1. How does the reported $R^2$ relate to the CV $R^2$?\n",
        "2. What does the reported RMSE mean?\n",
        "3. How does `sqft_living` relate to the `log_price` prediction? (Ask ChatGPT and see if it gets it right. The last time I tried it was close but not perfect.)\n",
        "\n",
        "A useful way to think about gauging how variables relate to predictions is just to probe the rule by predicting at some values of interest. This can be especially useful for black-box prediction rules.\n",
        "\n",
        "Let's specifically look at 2000 vs 2100 square feet and 4 vs 5 bedrooms"
      ],
      "metadata": {
        "id": "TTaNZEQYhdxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newX = pd.DataFrame(columns = X.columns)\n",
        "newX = newX._append(X.iloc[125]) # Observation with a 2.5 bathroom, 4 bedroom, 2000 square foot house\n",
        "newX = pd.DataFrame(np.repeat(newX.values, 4, axis=0))\n",
        "newX.columns = X.columns\n",
        "newX.iloc[1,5] = 2100\n",
        "newX.iloc[1,6] = 2100**2\n",
        "newX.iloc[1,22] = 4*2100\n",
        "newX.iloc[1,23] = 2.5*2100\n",
        "newX.iloc[2,3] = 5\n",
        "newX.iloc[2,4] = 5**2\n",
        "newX.iloc[2,22] = 5*2000\n",
        "newX.iloc[3,3] = 5\n",
        "newX.iloc[3,4] = 5**2\n",
        "newX.iloc[3,5] = 2100\n",
        "newX.iloc[3,6] = 2100**2\n",
        "newX.iloc[3,22] = 5*2100\n",
        "newX.iloc[3,23] = 2.5*2100\n",
        "\n",
        "# Predicted log prices for our four hypothetical houses\n",
        "yhat = result.predict(newX)\n",
        "print('Difference in predicted log(price) 2100 vs 2000 sqft (4 beds):', yhat[1]-yhat[0])\n",
        "print('Difference in predicted log(price) 2100 vs 2000 sqft (5 beds):', yhat[3]-yhat[2])\n",
        "print('Difference in predicted log(price) 4 vs 5 beds (2000 sqft):', yhat[2]-yhat[0])\n",
        "print('Difference in predicted log(price) 4 vs 5 beds (2100 sqft):', yhat[3]-yhat[1])\n",
        "print('\\n')\n",
        "print('Difference in predicted price 2100 vs 2000 sqft (4 beds):', np.exp(yhat[1])-np.exp(yhat[0]))\n",
        "print('Difference in predicted price 2100 vs 2000 sqft (5 beds):', np.exp(yhat[3])-np.exp(yhat[2]))\n",
        "print('Difference in predicted price 4 vs 5 beds (2000 sqft):', np.exp(yhat[2])-np.exp(yhat[0]))\n",
        "print('Difference in predicted price 4 vs 5 beds (2100 sqft):', np.exp(yhat[3])-np.exp(yhat[1]))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "sAfWUw9Mvhlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline linear models: LAV regression\n",
        "\n",
        "We can run least absolute values regression using `sklearn`'s `QuantileRegressor`"
      ],
      "metadata": {
        "id": "hXHdRh2sykGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import QuantileRegressor\n",
        "\n",
        "model = make_pipeline(StandardScaler(), QuantileRegressor(alpha = 0, solver = 'highs-ds'))\n",
        "# Lots of learners work best with variables on a common scale. StandardScaler()\n",
        "# standardizes the data. The pipeline is going to standardize before running\n",
        "# the quantile regression.\n",
        "\n",
        "# Cross validation.\n",
        "baseqr = cross_validate(model, X, y.to_numpy().ravel(),\n",
        "                       scoring = ('neg_mean_squared_error','neg_mean_absolute_error'), cv=cvsplit)\n",
        "print('Baseline LAV: CV RMSE: {m1:=.2f}; CV R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=np.sqrt(-baseqr['test_neg_mean_squared_error'].mean()),\n",
        "          m2=1-(-baseqr['test_neg_mean_squared_error'].mean()/benchMSE),\n",
        "          m3=p_base))\n",
        "\n",
        "print('Baseline LAV: CV MAE: {m1:=.2f}; CV MAE R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=-baseqr['test_neg_mean_absolute_error'].mean(),\n",
        "          m2=1-(-baseqr['test_neg_mean_absolute_error'].mean()/benchMAE),\n",
        "          m3=p_base))\n",
        "\n",
        "#####################################################################\n",
        "# Model for log-price\n",
        "# Set things up for custom scoring when using log(price)\n",
        "scoring = {'expmse': expmse_score,\n",
        "           'expmae': expmae_score}\n",
        "\n",
        "# Cross validation.\n",
        "baselogqr = cross_validate(model, X, logy,\n",
        "                       scoring = scoring, cv=cvsplit)\n",
        "print('Baseline logLAV: CV RMSE: {m1:=.2f}; CV R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=np.sqrt(-baselogqr['test_expmse'].mean()),\n",
        "          m2=1-(-baselogqr['test_expmse'].mean()/benchMSE),\n",
        "          m3=p_base))\n",
        "\n",
        "print('Baseline logLAV: CV MAE: {m1:=.2f}; CV MAE R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=-baselogqr['test_expmae'].mean(),\n",
        "          m2=1-(-baselogqr['test_expmae'].mean()/benchMAE),\n",
        "          m3=p_base))"
      ],
      "metadata": {
        "id": "sT_yBzsqy4g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add these results to our result table\n",
        "model_results = model_results._append({'Model': 'Baseline LAV',\n",
        "                                       'RMSE': np.sqrt(-baseqr['test_neg_mean_squared_error'].mean()),\n",
        "                                       'R2 - MSE': 1-(-baseqr['test_neg_mean_squared_error'].mean()/benchMSE),\n",
        "                                       'MAE': -baseqr['test_neg_mean_absolute_error'].mean(),\n",
        "                                       'R2 - MAE': 1-(-baseqr['test_neg_mean_absolute_error'].mean()/benchMAE),\n",
        "                                       'p': p_base,\n",
        "                                       'p_use': p_base}, ignore_index=True)\n",
        "\n",
        "model_results = model_results._append({'Model': 'Baseline logLAV',\n",
        "                                       'RMSE': np.sqrt(-baselogqr['test_expmse'].mean()),\n",
        "                                       'R2 - MSE': 1-(-baselogqr['test_expmse'].mean()/benchMSE),\n",
        "                                       'MAE': -baselogqr['test_expmae'].mean(),\n",
        "                                       'R2 - MAE': 1-(-baselogqr['test_expmae'].mean()/benchMAE),\n",
        "                                       'p': p_base,\n",
        "                                       'p_use': p_base}, ignore_index=True)\n",
        "\n",
        "model_results"
      ],
      "metadata": {
        "id": "TytACKrl380S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A More Flexible Model\n",
        "\n",
        "Maybe we haven't captured all the nonlinearity with our quadratic polynomials. Maybe there are interactions between variables besides the couple we thought about (`sqft_living` with `bedrooms` and `bathrooms`).\n",
        "\n",
        "Let's try a more elaborate model."
      ],
      "metadata": {
        "id": "TEmA4oEIV0gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify model as formula\n",
        "# Putting a variable inside \"C\" will make the variable be treated as categorical\n",
        "# by turning it into dummy variables\n",
        "# Putting a variable inside \"poly\" will a create a polynomial in that variable\n",
        "flex = (\"price ~ poly(bathrooms, degree=4, raw=True) + poly(bedrooms, degree=4, raw=True)\"\n",
        "             \" + poly(sqft_living, degree=6, raw=True) + poly(sqft_lot, degree=4, raw=True) + poly(floors, degree=4, raw=True)\"\n",
        "             \" + waterfront + C(view) + C(condition)\"\n",
        "             \" + poly(yr_built, degree=4, raw=True) + poly(yr_renovated, degree=4, raw=True) + C(renovated_flag) + C(basement_flag)\"\n",
        "             \" + poly(bathrooms, degree=4, raw=True):(poly(bedrooms, degree=4, raw=True)\"\n",
        "             \" + poly(sqft_living, degree=6, raw=True) + poly(sqft_lot, degree=4, raw=True) + poly(floors, degree=4, raw=True)\"\n",
        "             \" + waterfront + C(view) + C(condition)\"\n",
        "             \" + poly(yr_built, degree=4, raw=True) + poly(yr_renovated, degree=4, raw=True) + C(renovated_flag) + C(basement_flag))\"\n",
        "             \" + poly(bedrooms, degree=4, raw=True):(poly(sqft_living, degree=6, raw=True)\"\n",
        "             \" + poly(sqft_lot, degree=4, raw=True) + poly(floors, degree=4, raw=True)\"\n",
        "             \" + waterfront + C(view) + C(condition)\"\n",
        "             \" + poly(yr_built, degree=4, raw=True) + poly(yr_renovated, degree=4, raw=True) + C(renovated_flag) + C(basement_flag))\"\n",
        "             \" + poly(sqft_living, degree=6, raw=True):(poly(sqft_lot, degree=4, raw=True) + poly(floors, degree=4, raw=True)\"\n",
        "             \" + waterfront + C(view) + C(condition)\"\n",
        "             \" + poly(yr_built, degree=4, raw=True) + poly(yr_renovated, degree=4, raw=True) + C(renovated_flag) + C(basement_flag))\"\n",
        "             \" + poly(sqft_lot, degree=4, raw=True):(poly(floors, degree=4, raw=True)\"\n",
        "             \" + waterfront + C(view) + C(condition)\"\n",
        "             \" + poly(yr_built, degree=4, raw=True) + poly(yr_renovated, degree=4, raw=True) + C(renovated_flag) + C(basement_flag))\"\n",
        "             \" + poly(floors, degree=4, raw=True):(waterfront + C(view) + C(condition)\"\n",
        "             \" + poly(yr_built, degree=4, raw=True) + poly(yr_renovated, degree=4, raw=True) + C(renovated_flag) + C(basement_flag))\"\n",
        "             \" + waterfront:(C(view) + C(condition)\"\n",
        "             \" + poly(yr_built, degree=4, raw=True) + poly(yr_renovated, degree=4, raw=True) + C(renovated_flag) + C(basement_flag))\"\n",
        "             \" + C(view):(C(condition) + poly(yr_built, degree=4, raw=True) + poly(yr_renovated, degree=4, raw=True) + C(renovated_flag) + C(basement_flag))\"\n",
        "             \" + C(condition):(poly(yr_built, degree=4, raw=True) + poly(yr_renovated, degree=4, raw=True) + C(renovated_flag) + C(basement_flag))\"\n",
        "             \" + poly(yr_built, degree=4, raw=True):(poly(yr_renovated, degree=4, raw=True) + C(renovated_flag) + C(basement_flag))\"\n",
        "             \" + poly(yr_renovated, degree=4, raw=True):(C(renovated_flag) + C(basement_flag))\"\n",
        "             \" + C(renovated_flag):C(basement_flag)\")\n",
        "\n",
        "# Create variables based on model defined in formula\n",
        "y, X = model_matrix(flex, data)\n",
        "n, p_flex = X.shape\n",
        "\n",
        "model = make_pipeline(StandardScaler(), LinearRegression())\n",
        "# Lots of learners work best with variables on a common scale. StandardScaler()\n",
        "# standardizes the data. This can be very important with high order polynomials.\n",
        "\n",
        "# Cross validation.\n",
        "flexlm = cross_validate(model, X, y,\n",
        "                       scoring = ('neg_mean_squared_error','neg_mean_absolute_error'), cv=cvsplit)\n",
        "print('Flex LM: CV RMSE: {m1:=.2f}; CV R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=np.sqrt(-flexlm['test_neg_mean_squared_error'].mean()),\n",
        "          m2=1-(-flexlm['test_neg_mean_squared_error'].mean()/benchMSE),\n",
        "          m3=p_flex))\n",
        "\n",
        "print('Flex LM: CV MAE: {m1:=.2f}; CV MAE R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=-flexlm['test_neg_mean_absolute_error'].mean(),\n",
        "          m2=1-(-flexlm['test_neg_mean_absolute_error'].mean()/benchMAE),\n",
        "          m3=p_flex))\n",
        "\n",
        "#####################################################################\n",
        "# Model for log-price\n",
        "logy = data['log_price']\n",
        "\n",
        "# Set things up for custom scoring when using log(price)\n",
        "scoring = {'expmse': expmse_score,\n",
        "           'expmae': expmae_score}\n",
        "\n",
        "# Cross validation. We're just going to look at MSE from here on out\n",
        "flexloglm = cross_validate(model, X, logy,\n",
        "                       scoring = scoring, cv=cvsplit)\n",
        "print('Flex LogLM: CV RMSE: {m1:=.2f}; CV R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=np.sqrt(-flexloglm['test_expmse'].mean()),\n",
        "          m2=1-(-flexloglm['test_expmse'].mean()/benchMSE),\n",
        "          m3=p_flex))\n",
        "\n",
        "print('Flex LogLM: CV MAE: {m1:=.2f}; CV MAE R2: {m2:=.3f}; p: {m3:=.0f}'\n",
        "  .format(m1=-flexloglm['test_expmae'].mean(),\n",
        "          m2=1-(-flexloglm['test_expmae'].mean()/benchMAE),\n",
        "          m3=p_flex))\n"
      ],
      "metadata": {
        "id": "xBxnPbJFWs-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aside:** I cheated in my definition of the scorer for the log models to prevent numeric issues in silly models (like this one). The actual performance is much worse.\n",
        "\n",
        "We're not going to add these performance metrics to our table because they're awful and make the table unreadable."
      ],
      "metadata": {
        "id": "nGXS7F_PRkWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the flexible model does a really bad job.\n",
        "\n",
        "**Questions:**\n",
        "1. I'm only showing validation sample performance measures. How do we think in-sample validation measures line up?\n",
        "2. Does the very poor validation performance of the \"interactions\" model mean nothing in the model adds predictive power beyond the other models?\n",
        "3. Are we sure we have tried everything we want? There are lots of \"intermediate\" models?\n",
        "4. Should we have even more interactions or allow for higher order nonlinearity? How do we specify all the choices?"
      ],
      "metadata": {
        "id": "sFe4KqD-kHh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso\n",
        "\n",
        "A popular approach to estimating linear models when there are many potential variables is the *Lasso*.\n",
        "\n",
        "Recall that conventional linear model estimates are obtained by solving\n",
        "\n",
        "$$\\min \\sum_{i} (y_i - b_0 - b_1 X_1 - ... - b_p X_p)^2$$\n",
        "\n",
        "where the sum is over all observations in the training data. That is, we try to find the linear prediction rule for $Y$ that is as close as possible to $Y$ in the training data.\n",
        "\n",
        "Lasso solves a similar problem\n",
        "\n",
        "$$\\min \\sum_{i} (y_i - b_0 - b_1 X_1 - ... - b_p X_p)^2 + \\lambda \\sum_j |b_j|.$$\n",
        "\n",
        "The key thing we have added is a *penalty term* $\\lambda \\sum_j |b_j|$. Intuitively, this penalty says that if you want to move a coefficient $b_j$ away from 0 to improve the in-sample fit, you also have to pay a cost introduced by the penalty term.\n",
        "\n",
        "In practice, the presence of the penalty may lead to variables being excluded from the model. Intuitively, if the benefit of moving a $b_j$ away from 0 is smaller than the cost, the coefficient will be left at 0.\n",
        "\n",
        "When we do Lasso, we have to choose the *tuning parameter* $\\lambda$ which controls the cost of increasing the size of coefficients. We choose $\\lambda$ by trying several values and choosing the best according to cross-validation (or just validation).\n",
        "\n",
        "[Aside: There are MANY other penalized regression procedures. Another popular one is Ridge. There's also grouped Lasso, Elastic Net, ... The basic idea of all of them is to guard against overfitting by penalizing large values of coefficients. Not all of them do variable selection.]\n"
      ],
      "metadata": {
        "id": "A38qnsL2krcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at how Lasso does in our example. We have to start from a model that has the universe of variables that could matter. We'll start from both the polynomial and interactive models and see what happens."
      ],
      "metadata": {
        "id": "hUdqnJ_xoCK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso with baseline model"
      ],
      "metadata": {
        "id": "_OZBz1R8o09o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to repeat the exercise but now use Lasso. Lasso requires us to choose how much we want it \"to cost\" to increase a parameter's magnitude (i.e. to move it away from zero). We're going to choose this by trying a bunch out and taking the best one. Ideally, we'd do this with another validation dataset."
      ],
      "metadata": {
        "id": "ZORFBIM873sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create variables from our baseline model (defined in formula base)\n",
        "y, X = model_matrix(base, data)\n",
        "n, p_base = X.shape\n",
        "\n",
        "# Sequence of penalty parameters to try\n",
        "ub = np.std(y)*np.sqrt(2*np.log(2*p_base/.05))/np.sqrt(n)\n",
        "lambdas = np.geomspace(start=ub/100, stop=ub, num=25)\n",
        "\n",
        "# Let's do cross-validation in the training data to choose a value for lambda\n",
        "model = make_pipeline(StandardScaler(), Lasso())\n",
        "# \"StandardScaler\" is important here. It standardizes the data. Because Lasso\n",
        "# depends on the coefficients of the linear model and the coefficients depend\n",
        "# on the scale of the X's, you can get very different answers depending on how\n",
        "# you choose to scale the variables.\n",
        "\n",
        "# Cross validation. Set up parameter we are trying to optimize.\n",
        "parameters = {'lasso__alpha':lambdas.ravel()}\n",
        "\n",
        "baselas = GridSearchCV(model, parameters,\n",
        "                       scoring = ('neg_mean_squared_error','neg_mean_absolute_error'),\n",
        "                       refit = 'neg_mean_squared_error', cv=cvsplit)\n",
        "baselas.fit(X, y)\n",
        "\n",
        "# Extract the performance measures and find the values at the MSE minimizing\n",
        "# value of the tuning parameter\n",
        "# Could also look at MAE minimizing, but didn't do it here.\n",
        "lranks = baselas.cv_results_.get('rank_test_neg_mean_squared_error')\n",
        "cvmse_baselas = -baselas.cv_results_.get('mean_test_neg_mean_squared_error')\n",
        "cvmae_baselas = -baselas.cv_results_.get('mean_test_neg_mean_absolute_error')\n",
        "\n",
        "bestmse_baselas = cvmse_baselas[lranks == 1].min()\n",
        "bestmae_baselas = cvmae_baselas[lranks == 1].min()\n",
        "\n",
        "# Let's see how many variables are lasso model is using\n",
        "# Get the best model\n",
        "best_model = baselas.best_estimator_\n",
        "\n",
        "# Access Lasso model from the pipeline\n",
        "lasso_model = best_model.named_steps['lasso']\n",
        "\n",
        "# Retrieve coefficients\n",
        "coefficients = lasso_model.coef_\n",
        "p_baselas_use = sum(coefficients != 0)\n",
        "\n",
        "# Print out summary of performance\n",
        "print('Baseline Lasso: CV RMSE: {m1:=.2f}; CV R2: {m2:=.3f}'\n",
        "  .format(m1=np.sqrt(bestmse_baselas),\n",
        "          m2=1-(bestmse_baselas/benchMSE)))\n",
        "\n",
        "print('Baseline Lasso: CV MAE: {m1:=.2f}; CV MAE R2: {m2:=.3f}'\n",
        "  .format(m1=bestmae_baselas,\n",
        "          m2=1-(bestmae_baselas/benchMAE)))\n",
        "\n",
        "print(\"p: \", p_base)\n",
        "print(\"p_baselas_use: \", p_baselas_use)\n"
      ],
      "metadata": {
        "id": "K25yDEpvuZ_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add these results to our result table\n",
        "model_results = model_results._append({'Model': 'Baseline Lasso',\n",
        "                                       'RMSE': np.sqrt(bestmse_baselas),\n",
        "                                       'R2 - MSE': 1-(bestmse_baselas/benchMSE),\n",
        "                                       'MAE': bestmae_baselas,\n",
        "                                       'R2 - MAE': 1-(bestmae_baselas/benchMAE),\n",
        "                                       'p': p_base,\n",
        "                                       'p_use': p_baselas_use}, ignore_index=True)\n",
        "\n",
        "model_results"
      ],
      "metadata": {
        "id": "SQhUeUXzLxjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Which variables do we use\n",
        "print(X.columns[lasso_model.coef_ != 0])\n",
        "\n",
        "# Which do we drop\n",
        "print(X.columns[lasso_model.coef_ == 0])\n"
      ],
      "metadata": {
        "id": "yBCeFxWO6q_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the performance metrics across values of the tuning parameter\n",
        "cvrmse_baselas = np.sqrt(cvmse_baselas)\n",
        "lambdas = baselas.cv_results_.get('param_lasso__alpha').tolist()\n",
        "\n",
        "# Make plot\n",
        "plt.semilogx(lambdas, cvrmse_baselas, label='CV RMSE')\n",
        "plt.semilogx(lambdas, cvmae_baselas, label='CV MAE')\n",
        "plt.axvline(baselas.best_params_.get('lasso__alpha'),\n",
        "            linestyle=\"--\", color=\"black\", label=\"CV estimate\")\n",
        "plt.xlabel(r\"$\\lambda$\")\n",
        "plt.ylabel(\"Error Estimate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C7voNbDrzcy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's change and look at `log_price` as the dependent variable."
      ],
      "metadata": {
        "id": "yYElwtbAwIUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log price model\n",
        "\n",
        "# Sequence of penalty parameters to try\n",
        "ub = np.std(logy)*np.sqrt(2*np.log(2*p_base/.05))/np.sqrt(n)\n",
        "lambdas = np.geomspace(start=ub/100, stop=ub, num=25)\n",
        "\n",
        "# Let's do cross-validation in the training data to choose a value for lambda\n",
        "model = make_pipeline(StandardScaler(), Lasso())\n",
        "# \"StandardScaler\" is important here. It standardizes the data. Because Lasso\n",
        "# depends on the coefficients of the linear model and the coefficients depend\n",
        "# on the scale of the X's, you can get very different answers depending on how\n",
        "# you choose to scale the variables.\n",
        "\n",
        "# Cross validation\n",
        "parameters = {'lasso__alpha':lambdas.ravel()}\n",
        "\n",
        "# Set things up for custom scoring when using log(price)\n",
        "scoring = {'expmse': expmse_score,\n",
        "           'expmae': expmae_score,\n",
        "           'mse': 'neg_mean_squared_error',\n",
        "           'mae': 'neg_mean_absolute_error'}\n",
        "\n",
        "baseloglas = GridSearchCV(model, parameters,\n",
        "                       scoring = scoring,\n",
        "                       refit = 'expmse', cv=cvsplit)\n",
        "\n",
        "baseloglas.fit(X, logy)\n",
        "\n",
        "# Extract the performance measures and find the values at the MSE minimizing\n",
        "# value of the tuning parameter. Note that we are looking at MSE for PRICE!, not\n",
        "# log(price)\n",
        "# Could also look at MAE minimizing, but didn't do it here.\n",
        "lranks = baseloglas.cv_results_.get('rank_test_expmse')\n",
        "cvlogmse_baseloglas = -baseloglas.cv_results_.get('mean_test_mse')\n",
        "cvlogmae_baseloglas = -baseloglas.cv_results_.get('mean_test_mae')\n",
        "cvmse_baseloglas = -baseloglas.cv_results_.get('mean_test_expmse')\n",
        "cvmae_baseloglas = -baseloglas.cv_results_.get('mean_test_expmae')\n",
        "\n",
        "bestmse_baseloglas = cvmse_baseloglas[lranks == 1].min()\n",
        "bestmae_baseloglas = cvmae_baseloglas[lranks == 1].min()\n",
        "\n",
        "# Let's see how many variables are lasso model is using\n",
        "# Get the best model\n",
        "best_model = baseloglas.best_estimator_\n",
        "\n",
        "# Access Lasso model from the pipeline\n",
        "lasso_model = best_model.named_steps['lasso']\n",
        "\n",
        "# Retrieve coefficients\n",
        "coefficients = lasso_model.coef_\n",
        "p_baseloglas_use = sum(coefficients != 0)\n",
        "\n",
        "# Print out summary of performance\n",
        "print('Log Poly Lasso: CV RMSE: {m1:=.2f}; CV R2: {m2:=.3f}'\n",
        "  .format(m1=np.sqrt(bestmse_baseloglas),\n",
        "          m2=1-(bestmse_baseloglas/benchMSE)))\n",
        "\n",
        "print('Log Poly Lasso: CV MAE: {m1:=.2f}; CV MAE R2: {m2:=.3f}'\n",
        "  .format(m1=bestmae_baseloglas,\n",
        "          m2=1-(bestmae_baseloglas/benchMAE)))\n",
        "\n",
        "print(\"p: \", p_base)\n",
        "print(\"p_baseloglas_use: \", p_baseloglas_use)"
      ],
      "metadata": {
        "id": "CtcXgoE88dfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add these results to our result table\n",
        "model_results = model_results._append({'Model': 'Log Poly Lasso',\n",
        "                                       'RMSE': np.sqrt(bestmse_baseloglas),\n",
        "                                       'R2 - MSE': 1-(bestmse_baseloglas/benchMSE),\n",
        "                                       'MAE': bestmae_baseloglas,\n",
        "                                       'R2 - MAE': 1-(bestmae_baseloglas/benchMAE),\n",
        "                                       'p': p_base,\n",
        "                                       'p_use': p_baseloglas_use}, ignore_index=True)\n",
        "\n",
        "model_results"
      ],
      "metadata": {
        "id": "0HLK-J_cMGgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the performance metrics across values of the tuning parameter\n",
        "cvlogrmse_baseloglas = np.sqrt(cvlogmse_baseloglas)\n",
        "lambdas = baseloglas.cv_results_.get('param_lasso__alpha').tolist()\n",
        "\n",
        "# Make plot\n",
        "plt.semilogx(lambdas, cvlogrmse_baseloglas, label='CV RMSE')\n",
        "plt.semilogx(lambdas, cvlogmae_baseloglas, label='CV MAE')\n",
        "plt.axvline(baseloglas.best_params_.get('lasso__alpha'),\n",
        "            linestyle=\"--\", color=\"black\", label=\"CV estimate\")\n",
        "plt.xlabel(r\"$\\lambda$\")\n",
        "plt.ylabel(\"Error Estimate\")\n",
        "plt.title(\"RMSE and MAE for log(price)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fpS05i3JN3Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the performance metrics for PRICE across values of the tuning parameter\n",
        "cvrmse_baseloglas = np.sqrt(cvmse_baseloglas)\n",
        "lambdas = baseloglas.cv_results_.get('param_lasso__alpha').tolist()\n",
        "\n",
        "# Make plot\n",
        "plt.semilogx(lambdas, cvrmse_baseloglas, label='CV RMSE')\n",
        "plt.semilogx(lambdas, cvmae_baseloglas, label='CV MAE')\n",
        "plt.axvline(baseloglas.best_params_.get('lasso__alpha'),\n",
        "            linestyle=\"--\", color=\"black\", label=\"CV estimate\")\n",
        "plt.xlabel(r\"$\\lambda$\")\n",
        "plt.ylabel(\"Error Estimate\")\n",
        "plt.title(\"RMSE and MAE for price\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k6YFXgmRTYlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables in the baseline log model estimated via lasso and estimated coefficients\n",
        "baseloglas_coefs = pd.DataFrame(data={'Variable Names': X.columns[lasso_model.coef_ != 0], 'Coefficient': lasso_model.coef_[lasso_model.coef_ != 0]})\n",
        "print(baseloglas_coefs.to_markdown())"
      ],
      "metadata": {
        "id": "WZdzujUiAuhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso with flexible model\n",
        "\n",
        "**(This block takes ~ 5 minutes to run.)**"
      ],
      "metadata": {
        "id": "Ndr5MDAXwT-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Price prediction\n",
        "\n",
        "# Create variables based on model defined in formula\n",
        "y, X = model_matrix(flex, data)\n",
        "n, p_flex = X.shape\n",
        "\n",
        "# Sequence of penalty parameters to try\n",
        "ub = np.std(y)*np.sqrt(2*np.log(2*p_flex/.05))/np.sqrt(n)\n",
        "lambdas = np.geomspace(start=ub/10, stop=ub, num=15)\n",
        "\n",
        "# Let's do cross-validation in the training data to choose a value for lambda\n",
        "model = make_pipeline(StandardScaler(), Lasso())\n",
        "# \"StandardScaler\" is important here. It standardizes the data. Because Lasso\n",
        "# depends on the coefficients of the linear model and the coefficients depend\n",
        "# on the scale of the X's, you can get very different answers depending on how\n",
        "# you choose to scale the variables.\n",
        "\n",
        "# Cross validation\n",
        "parameters = {'lasso__alpha':lambdas.ravel()}\n",
        "\n",
        "flexlas = GridSearchCV(model, parameters,\n",
        "                       scoring = ('neg_mean_squared_error','neg_mean_absolute_error'),\n",
        "                       refit = 'neg_mean_squared_error', cv=cvsplit)\n",
        "flexlas.fit(X, y)\n",
        "\n",
        "# Extract the performance measures and find the values at the MSE minimizing\n",
        "# value of the tuning parameter\n",
        "# Could also look at MAE minimizing, but didn't do it here.\n",
        "lranks = flexlas.cv_results_.get('rank_test_neg_mean_squared_error')\n",
        "cvmse_flexlas = -flexlas.cv_results_.get('mean_test_neg_mean_squared_error')\n",
        "cvmae_flexlas = -flexlas.cv_results_.get('mean_test_neg_mean_absolute_error')\n",
        "\n",
        "bestmse_flexlas = cvmse_flexlas[lranks == 1].min()\n",
        "bestmae_flexlas = cvmae_flexlas[lranks == 1].min()\n",
        "\n",
        "# Let's see how many variables are lasso model is using\n",
        "# Get the best model\n",
        "best_model = flexlas.best_estimator_\n",
        "\n",
        "# Access Lasso model from the pipeline\n",
        "lasso_model = best_model.named_steps['lasso']\n",
        "\n",
        "# Retrieve coefficients\n",
        "coefficients = lasso_model.coef_\n",
        "p_flexlas_use = sum(coefficients != 0)\n",
        "\n",
        "# Print out summary of performance\n",
        "print('Level Interactions Lasso: CV RMSE: {m1:=.2f}; CV R2: {m2:=.3f}'\n",
        "  .format(m1=np.sqrt(bestmse_flexlas),\n",
        "          m2=1-(bestmse_flexlas/benchMSE)))\n",
        "\n",
        "print('Level Interactions Lasso: CV MAE: {m1:=.2f}; CV MAE R2: {m2:=.3f}'\n",
        "  .format(m1=bestmae_flexlas,\n",
        "          m2=1-(bestmae_flexlas/benchMAE)))\n",
        "\n",
        "print(\"p_int: \", p_flex)\n",
        "print(\"p_flexlas_use: \", p_flexlas_use)\n"
      ],
      "metadata": {
        "id": "HpZXgl7V44LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the performance metrics across values of the tuning parameter\n",
        "cvrmse_flexlas = np.sqrt(cvmse_flexlas)\n",
        "lambdas = flexlas.cv_results_.get('param_lasso__alpha').tolist()\n",
        "\n",
        "# Make plot\n",
        "plt.semilogx(lambdas, cvrmse_flexlas, label='CV RMSE')\n",
        "plt.semilogx(lambdas, cvmae_flexlas, label='CV MAE')\n",
        "plt.axvline(flexlas.best_params_.get('lasso__alpha'),\n",
        "            linestyle=\"--\", color=\"black\", label=\"CV estimate\")\n",
        "plt.xlabel(r\"$\\lambda$\")\n",
        "plt.ylabel(\"Error Estimate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eP7Ia82m_hJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add these results to our result table\n",
        "model_results = model_results._append({'Model': 'Level Interactions Lasso',\n",
        "                                       'RMSE': np.sqrt(bestmse_flexlas),\n",
        "                                       'R2 - MSE': 1-(bestmse_flexlas/benchMSE),\n",
        "                                       'MAE': bestmae_flexlas,\n",
        "                                       'R2 - MAE': 1-(bestmae_flexlas/benchMAE),\n",
        "                                       'p': p_flex,\n",
        "                                       'p_use': p_flexlas_use}, ignore_index=True)\n",
        "\n",
        "model_results"
      ],
      "metadata": {
        "id": "FrhsdpXXM1xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables in the flexible model estimated via lasso and estimated coefficients\n",
        "flexlas_coefs = pd.DataFrame(data={'Variable Names': X.columns[lasso_model.coef_ != 0], 'Coefficient': lasso_model.coef_[lasso_model.coef_ != 0]})\n",
        "print(flexlas_coefs.to_markdown())"
      ],
      "metadata": {
        "id": "VYCYF7fnbSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "1. We have a list of variables and coefficients. Is the model particularly interpretable?\n",
        "2. Should we conclude we have the \"correct\" variables?"
      ],
      "metadata": {
        "id": "hwfJNxRpAWbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's finish this section by looking at `log_price`"
      ],
      "metadata": {
        "id": "phBj1xQYb2lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# log price prediction\n",
        "\n",
        "# Sequence of penalty parameters to try\n",
        "ub = np.std(logy)*np.sqrt(2*np.log(2*p_flex/.05))/np.sqrt(n)\n",
        "lambdas = np.geomspace(start=ub/10, stop=ub, num=15)\n",
        "\n",
        "# Let's do cross-validation in the training data to choose a value for lambda\n",
        "model = make_pipeline(StandardScaler(), Lasso())\n",
        "# \"StandardScaler\" is important here. It standardizes the data. Because Lasso\n",
        "# depends on the coefficients of the linear model and the coefficients depend\n",
        "# on the scale of the X's, you can get very different answers depending on how\n",
        "# you choose to scale the variables.\n",
        "\n",
        "# Cross validation\n",
        "parameters = {'lasso__alpha':lambdas.ravel()}\n",
        "\n",
        "scoring = {'expmse': expmse_score,\n",
        "           'expmae': expmae_score,\n",
        "           'mse': 'neg_mean_squared_error',\n",
        "           'mae': 'neg_mean_absolute_error'}\n",
        "\n",
        "flexloglas = GridSearchCV(model, parameters,\n",
        "                       scoring = scoring,\n",
        "                       refit = 'expmse', cv=cvsplit)\n",
        "flexloglas.fit(X, logy)\n",
        "\n",
        "# Extract the performance measures and find the values at the MSE minimizing\n",
        "# value of the tuning parameter. MSE based on predicting PRICE not LOGPRICE!\n",
        "lranks = flexloglas.cv_results_.get('rank_test_expmse')\n",
        "cvlogmse_flexloglas = -flexloglas.cv_results_.get('mean_test_mse')\n",
        "cvlogmae_flexloglas = -flexloglas.cv_results_.get('mean_test_mae')\n",
        "cvmse_flexloglas = -flexloglas.cv_results_.get('mean_test_expmse')\n",
        "cvmae_flexloglas = -flexloglas.cv_results_.get('mean_test_expmae')\n",
        "\n",
        "bestmse_flexloglas = cvmse_flexloglas[lranks == 1].min()\n",
        "bestmae_flexloglas = cvmae_flexloglas[lranks == 1].min()\n",
        "\n",
        "# Let's see how many variables are lasso model is using\n",
        "# Get the best model\n",
        "best_model = flexloglas.best_estimator_\n",
        "\n",
        "# Access Lasso model from the pipeline\n",
        "lasso_model = best_model.named_steps['lasso']\n",
        "\n",
        "# Retrieve coefficients\n",
        "coefficients = lasso_model.coef_\n",
        "p_flexloglas_use = sum(coefficients != 0)\n",
        "\n",
        "# Print out summary of performance\n",
        "print('Log Poly Lasso: CV RMSE: {m1:=.2f}; CV R2: {m2:=.3f}'\n",
        "  .format(m1=np.sqrt(bestmse_flexloglas),\n",
        "          m2=1-(bestmse_flexloglas/benchMSE)))\n",
        "\n",
        "print('Log Poly Lasso: CV MAE: {m1:=.2f}; CV MAE R2: {m2:=.3f}'\n",
        "  .format(m1=bestmae_flexloglas,\n",
        "          m2=1-(bestmae_flexloglas/benchMAE)))\n",
        "\n",
        "print(\"p: \", p_flex)\n",
        "print(\"p_flexloglas_use: \", p_flexloglas_use)"
      ],
      "metadata": {
        "id": "ZM4Vvtbkb1a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the performance metrics for PRICE across values of the tuning parameter\n",
        "cvrmse_flexloglas = np.sqrt(cvmse_flexloglas)\n",
        "lambdas = flexloglas.cv_results_.get('param_lasso__alpha').tolist()\n",
        "\n",
        "# Make plot\n",
        "plt.semilogx(lambdas, cvrmse_flexloglas, label='CV RMSE')\n",
        "plt.semilogx(lambdas, cvmae_flexloglas, label='CV MAE')\n",
        "plt.axvline(flexloglas.best_params_.get('lasso__alpha'),\n",
        "            linestyle=\"--\", color=\"black\", label=\"CV estimate\")\n",
        "plt.xlabel(r\"$\\lambda$\")\n",
        "plt.ylabel(\"Error Estimate\")\n",
        "plt.title(\"RMSE and MAE for price\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_cxrqceUcxaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add these results to our result table\n",
        "model_results = model_results._append({'Model': 'Log Interactions Lasso',\n",
        "                                       'RMSE': np.sqrt(bestmse_flexloglas),\n",
        "                                       'R2 - MSE': 1-(bestmse_flexloglas/benchMSE),\n",
        "                                       'MAE': bestmae_flexloglas,\n",
        "                                       'R2 - MAE': 1-(bestmae_flexloglas/benchMAE),\n",
        "                                       'p': p_flex,\n",
        "                                       'p_use': p_flexloglas_use}, ignore_index=True)\n",
        "\n",
        "model_results"
      ],
      "metadata": {
        "id": "vv1C5grQdRj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables in the flexible log model estimated via lasso and estimated coefficients\n",
        "flexloglas_coefs = pd.DataFrame(data={'Variable Names': X.columns[lasso_model.coef_ != 0], 'Coefficient': lasso_model.coef_[lasso_model.coef_ != 0]})\n",
        "print(flexloglas_coefs.to_markdown())"
      ],
      "metadata": {
        "id": "Y5lJx_iBc6Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions:**\n",
        "\n",
        "1. We have a list of variables and coefficients. Is the model particularly interpretable?\n",
        "2. Should we conclude we have the \"correct\" variables?\n",
        "3. What if there are things we didn't try? How far should we go? Maybe we should have looked at higher order interactions (i.e. multiplying more than two variables together), other transformations, ...\n",
        "\n",
        "**Miscellaneous comments:**\n",
        "\n",
        "When we moved to Lasso, we could have included `sqft_above` and `sqft_basement` in the model along with `sqft_living` and let the data decide which are more useful. (For example, maybe basement square feet are not as valuable as above ground square feet, which can't be picked up when only `sqft_living` is included.)\n",
        "\n",
        "We haven't considered `statezip` or `city` yet.\n",
        "\n",
        "Penalized quantile regression (e.g. \"lasso for LAV\") exists and is available in `sklearn`'s `QuantileRegressor`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "incLsjWDdgXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Comment on Linear Models**\n",
        "\n",
        "Linear (and related) models are useful, but they do not do *feature learning*. I.e. they rely on the analyst to specify all the relevant features **and their transformations** before building the predictive model. In settings where there is strong intuition for what is important and how it relates to the target, this is not a huge problem. In setting with limited data, simple models (e.g. ignore transformations of continuous regressors and just include them as is) tend to work well.\n",
        "\n",
        "Rather than require pre-specification of all input features, most ML methods leverage the power of machines and clever algorithms to simultaneously **learn feature representations** and build the model for making predictions.\n",
        "\n",
        "The ability of ML algorithms to learn feature representations without needing prespecification is practically very important in complex environments or settings with large amounts of data. Even in our relatively simple house price example, we can already see that relying on prespecifying everything can be daunting.\n",
        "\n",
        "Often (though not always) these models outperform methods that do not do feature learning in terms of predictive performance. Even if they do not \"beat\" more traditional methods, they almost always come with the benefit of alleviating the analyst's burden of choice."
      ],
      "metadata": {
        "id": "PhYex1DXhVK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Trees\n",
        "\n",
        "Rather than try to build all the relevant features ourselves. Let's fit a regression *tree* instead.\n",
        "\n",
        "The basic idea of a regression tree is to chop the data into rectangles based on the raw input variables and then fit a simple model - we'll just use the sample mean.\n",
        "\n",
        "The way we'll chop things into rectangles is by making binary splits of the data. By making chops of the data based on binary decisions, we simultaneously uncover nonlinearity (including interactions) and which variables are most informative in making our predictions. We do not have to prespecify the types of nonlinearity we think might matter, we just learn it from the data.\n",
        "\n",
        "Rather than try to talk about it, it's probably easier to just see how it works in our little example.\n",
        "\n"
      ],
      "metadata": {
        "id": "cerbcZ_XzuU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by pulling together a dataset that we will use for our trees. We'll focus on the same variables we used in our linear models. Note that we'll leave `sqft_above` and `sqft_basement` in as the tree can choose which representation of square footage it wants to use to best predict the outcome."
      ],
      "metadata": {
        "id": "_g6_LEPXjnIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tree = data.drop(columns=['city','statezip','renovated_flag','basement_flag'])\n",
        "\n",
        "# Let's make our outcome variables and predictors\n",
        "X = data_tree.drop(columns=['price','log_price'])\n",
        "price = data_tree['price']\n",
        "lprice = data_tree['log_price']"
      ],
      "metadata": {
        "id": "OQo8jId7IMWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we're just illustrating, we'll start by making a tree with just one split using the full data set."
      ],
      "metadata": {
        "id": "QDg3HYapJJty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's fit a tree to price with 1 split = 2 leaves\n",
        "tree1 = DecisionTreeRegressor(max_leaf_nodes = 2)\n",
        "tree1.fit(X, price)\n",
        "plot_tree(tree1, feature_names=X.columns)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FdtEVC6nNa9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first split in our tree is at `sqft_living` = 2915. We predict houses less than or equal to 2915 square feet to have price \\$457,977 and houses with more than 2915 square feet to have price \\$993,690. (The predictions are just the sample mean price of houses smaller than 2915 square feet and sample mean of price of houses larger than 2915 square feet.)\n",
        "\n",
        "This split was determined by looking at every possible split of every possible variable and asking which split provides the best prediction rule for `price` in the terms of having the minimum MSE.\n",
        "\n",
        "What happens when we want to have a prediction rule with 3 terminal nodes?"
      ],
      "metadata": {
        "id": "eypRN6H0kVzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's fit a tree with 2 splits = 3 leaves\n",
        "tree2 = DecisionTreeRegressor(max_leaf_nodes = 3)\n",
        "tree2.fit(X, y)\n",
        "plot_tree(tree2, feature_names=X.columns)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YPO7O58jN5bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that we have introduced another split along `sqft_living`. That we would split again on this variable was not predetermined. We tried every possible split along every possible variable of the subsample of houses with `sqft_living` $\\leq$ 2915 and every possible split along every possible variable of the subsample of houses with `sqft_living` $>$ 2915. Of all these possible choices, the best split, in the sense of reducing MSE of predicting `price`, was to split again at `sqft_living` = 6140.\n",
        "\n",
        "Our prediction rule now has three points, houses $\\leq$ 2915 square feet are predicted to have price \\$457,977. Houses with 2915 $<$ `sqft_living` $\\leq$ 6140 are predicted to have price \\$954,056. Houses greater than 6140 square feet are predicted to have price \\$2,873,812.\n",
        "\n",
        "We can, of course, keep going.\n"
      ],
      "metadata": {
        "id": "Pyq2kiYTldFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the figure bigger than default so it's easier to read\n",
        "width = 10\n",
        "height = 7\n",
        "plt.figure(figsize=(width, height))\n",
        "\n",
        "# Let's fit a tree with 3 splits = 4 leaves\n",
        "tree3 = DecisionTreeRegressor(max_leaf_nodes = 4)\n",
        "tree3.fit(X, y)\n",
        "plot_tree(tree3, feature_names=X.columns)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bVCJ4mGNRb9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make one more plot to show how the regression tree is uncovering nonlinearity without having to prespecify transformations of the features."
      ],
      "metadata": {
        "id": "9M6TV925WNhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "yfit = tree3.predict(X)\n",
        "xplot = np.sort(X['sqft_living'])\n",
        "yplot = np.sort(yfit)\n",
        "\n",
        "# Plot\n",
        "plt.figure()\n",
        "plt.plot(xplot, yplot, linewidth=2)\n",
        "plt.xlabel(\"Square Footage\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Decision Tree Regression with Four Leaves\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GHAwD6EHWX6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic tree models correspond to *step functions*. Step functions are simple, though not necessarily the best for lots of outcomes where we think *continuity* makes sense. We'll worry about this later."
      ],
      "metadata": {
        "id": "nk14AtqznT7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at one more larger tree to see how we also pick up interactions."
      ],
      "metadata": {
        "id": "6UoECI_TnmDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the figure bigger than default so it's easier to read\n",
        "width = 14\n",
        "height = 10\n",
        "plt.figure(figsize=(width, height))\n",
        "\n",
        "# Let's fit a tree with 8 leaves\n",
        "tree8 = DecisionTreeRegressor(max_leaf_nodes = 8)\n",
        "tree8.fit(X, y)\n",
        "plot_tree(tree8, feature_names=X.columns)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QmvCuQHomqQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can (and will) continue, though we'll stop showing the picture every time. The basic idea of trees is very simple. We keep make binary splits of the data where each split improves the fit to the training data. We can fit the training data very well with enough splits - though that's unlikely to generalize well.\n",
        "\n",
        "We choose which tree we want to use by (cross-) validation. There are lots of ways we could specify and train trees. Rather than look at number of leaves, we could look at depth. We might want to specify a minimum leaf size to avoid leaves with very few observations.\n",
        "\n",
        "We're just going to look at cross-validating over number of leaves and not worry about other details."
      ],
      "metadata": {
        "id": "ZUzv16fB1H8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter we want to choose based on cross-validation performance - number of leaves\n",
        "parameters = {'max_leaf_nodes':range(2,101)}\n",
        "\n",
        "scoring = {'mse': 'neg_mean_squared_error',\n",
        "           'mae': 'neg_mean_absolute_error'}\n",
        "\n",
        "tree = DecisionTreeRegressor()\n",
        "base_tree = GridSearchCV(tree, parameters, scoring = scoring, cv = cvsplit, refit = 'mse')\n",
        "base_tree.fit(X, price)\n",
        "\n",
        "# Plot CV performance\n",
        "leaves = base_tree.cv_results_.get('param_max_leaf_nodes')\n",
        "leaves = leaves.tolist()\n",
        "\n",
        "lranks = base_tree.cv_results_.get('rank_test_mse')\n",
        "cvmse_tree = -base_tree.cv_results_.get('mean_test_mse')\n",
        "cvmae_tree = -base_tree.cv_results_.get('mean_test_mae')\n",
        "\n",
        "bestmse_base_tree = cvmse_tree[lranks == 1].min()\n",
        "bestmae_base_tree = cvmae_tree[lranks == 1].min()\n",
        "\n",
        "plt.plot(leaves, np.sqrt(cvmse_tree), label = 'RMSE')\n",
        "plt.plot(leaves, cvmae_tree, label = 'MAE')\n",
        "plt.axvline(base_tree.best_params_.get('max_leaf_nodes'),\n",
        "            linestyle=\"--\", color=\"black\", label=\"CV estimate\")\n",
        "plt.xlabel(\"Number of leaves\")\n",
        "plt.ylabel(\"Cross-validation RMSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3hep1h1VjTRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add these results to our results table\n",
        "model_results = model_results._append({'Model': 'Decision Tree',\n",
        "                                       'RMSE': np.sqrt(bestmse_base_tree),\n",
        "                                       'R2 - MSE': 1-(bestmse_base_tree/benchMSE),\n",
        "                                       'MAE': bestmae_base_tree,\n",
        "                                       'R2 - MAE': 1-(bestmae_base_tree/benchMAE),\n",
        "                                       'p': '-',\n",
        "                                       'p_use': base_tree.best_params_.get('max_leaf_nodes')},\n",
        "                                       ignore_index=True)\n",
        "\n",
        "model_results"
      ],
      "metadata": {
        "id": "pG2LQlgNR06j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does our estimated tree look like?"
      ],
      "metadata": {
        "id": "7gTxMxq4ReWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "width = 15\n",
        "height = 15\n",
        "plt.figure(figsize=(width, height))\n",
        "\n",
        "# Let's plot the cv minimizing tree\n",
        "plot_tree(base_tree.best_estimator_, feature_names=X.columns)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zYdHsv-HRblX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We'd need a pretty big screen to see what's going on here.**"
      ],
      "metadata": {
        "id": "bhlw8ChyyxlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like lots of results with similar performance and highly variable across number of leaves. Default behavior in the software allows for very small leaves. For example, we can see from the estimated tree that we have leaves with just one observation. We might want to add more stability be ruling out small leaves."
      ],
      "metadata": {
        "id": "HYn0rol8rq2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter we want to choose based on cross-validation performance - number of leaves\n",
        "parameters = {'max_leaf_nodes':range(2,101)}\n",
        "\n",
        "scoring = {'mse': 'neg_mean_squared_error',\n",
        "           'mae': 'neg_mean_absolute_error'}\n",
        "\n",
        "# Let's make it so each leaf must have 10 or more observations\n",
        "tree = DecisionTreeRegressor(min_samples_leaf=10)\n",
        "tree10 = GridSearchCV(tree, parameters, scoring = scoring, cv = cvsplit, refit = 'mse')\n",
        "tree10.fit(X, price)\n",
        "\n",
        "leaves = tree10.cv_results_.get('param_max_leaf_nodes')\n",
        "leaves = leaves.tolist()\n",
        "\n",
        "lranks = tree10.cv_results_.get('rank_test_mse')\n",
        "cvmse_tree = -tree10.cv_results_.get('mean_test_mse')\n",
        "cvmae_tree = -tree10.cv_results_.get('mean_test_mae')\n",
        "\n",
        "bestmse_tree10 = cvmse_tree[lranks == 1].min()\n",
        "bestmae_tree10 = cvmae_tree[lranks == 1].min()\n",
        "\n",
        "plt.plot(leaves, np.sqrt(cvmse_tree), label = 'RMSE')\n",
        "plt.plot(leaves, cvmae_tree, label = 'MAE')\n",
        "plt.axvline(tree10.best_params_.get('max_leaf_nodes'),\n",
        "            linestyle=\"--\", color=\"black\", label=\"CV estimate\")\n",
        "plt.xlabel(\"Number of leaves\")\n",
        "plt.ylabel(\"Cross-validation RMSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yYs6jn19ViSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add these results to our results table\n",
        "model_results = model_results._append({'Model': 'Decision Tree 10',\n",
        "                                       'RMSE': np.sqrt(bestmse_tree10),\n",
        "                                       'R2 - MSE': 1-(bestmse_tree10/benchMSE),\n",
        "                                       'MAE': bestmae_tree10,\n",
        "                                       'R2 - MAE': 1-(bestmae_tree10/benchMAE),\n",
        "                                       'p': '-',\n",
        "                                       'p_use': tree10.best_params_.get('max_leaf_nodes')},\n",
        "                                       ignore_index=True)\n",
        "\n",
        "model_results"
      ],
      "metadata": {
        "id": "VCtaMspNWY3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we did before, we can also use `log_price` as dependent variable."
      ],
      "metadata": {
        "id": "KBnP6KDUlJQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter we want to choose based on cross-validation performance - number of leaves\n",
        "parameters = {'max_leaf_nodes':range(2,151)}\n",
        "\n",
        "scoring = {'expmse': expmse_score,\n",
        "           'expmae': expmae_score,\n",
        "           'mse': 'neg_mean_squared_error',\n",
        "           'mae': 'neg_mean_absolute_error'}\n",
        "\n",
        "tree = DecisionTreeRegressor()\n",
        "log_tree = GridSearchCV(tree, parameters, scoring = scoring, cv = cvsplit, refit = 'expmse')\n",
        "log_tree.fit(X, lprice)\n",
        "\n",
        "# Plot CV performance\n",
        "leaves = log_tree.cv_results_.get('param_max_leaf_nodes')\n",
        "leaves = leaves.tolist()\n",
        "\n",
        "lranks = log_tree.cv_results_.get('rank_test_expmse')\n",
        "cvmse_tree = -log_tree.cv_results_.get('mean_test_expmse')\n",
        "cvmae_tree = -log_tree.cv_results_.get('mean_test_expmae')\n",
        "\n",
        "bestmse_log_tree = cvmse_tree[lranks == 1].min()\n",
        "bestmae_log_tree = cvmae_tree[lranks == 1].min()\n",
        "\n",
        "plt.plot(leaves, np.sqrt(cvmse_tree), label = 'RMSE')\n",
        "plt.plot(leaves, cvmae_tree, label = 'MAE')\n",
        "plt.axvline(log_tree.best_params_.get('max_leaf_nodes'),\n",
        "            linestyle=\"--\", color=\"black\", label=\"CV estimate\")\n",
        "plt.xlabel(\"Number of leaves\")\n",
        "plt.ylabel(\"Cross-validation RMSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4rheJ0TPliq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add these results to our results table\n",
        "model_results = model_results._append({'Model': 'Decision Tree - Log',\n",
        "                                       'RMSE': np.sqrt(bestmse_log_tree),\n",
        "                                       'R2 - MSE': 1-(bestmse_log_tree/benchMSE),\n",
        "                                       'MAE': bestmae_log_tree,\n",
        "                                       'R2 - MAE': 1-(bestmae_log_tree/benchMAE),\n",
        "                                       'p': '-',\n",
        "                                       'p_use': log_tree.best_params_.get('max_leaf_nodes')},\n",
        "                                       ignore_index=True)\n",
        "\n",
        "model_results"
      ],
      "metadata": {
        "id": "rKkZcEsMlt6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter we want to choose based on cross-validation performance - number of leaves\n",
        "parameters = {'max_leaf_nodes':range(2,151)}\n",
        "\n",
        "scoring = {'expmse': expmse_score,\n",
        "           'expmae': expmae_score,\n",
        "           'mse': 'neg_mean_squared_error',\n",
        "           'mae': 'neg_mean_absolute_error'}\n",
        "\n",
        "tree = DecisionTreeRegressor(min_samples_leaf=10)\n",
        "log_tree10 = GridSearchCV(tree, parameters, scoring = scoring, cv = cvsplit, refit = 'expmse')\n",
        "log_tree10.fit(X, lprice)\n",
        "\n",
        "# Plot CV performance\n",
        "leaves = log_tree10.cv_results_.get('param_max_leaf_nodes')\n",
        "leaves = leaves.tolist()\n",
        "\n",
        "lranks = log_tree10.cv_results_.get('rank_test_expmse')\n",
        "cvmse_tree = -log_tree10.cv_results_.get('mean_test_expmse')\n",
        "cvmae_tree = -log_tree10.cv_results_.get('mean_test_expmae')\n",
        "\n",
        "bestmse_log_tree10 = cvmse_tree[lranks == 1].min()\n",
        "bestmae_log_tree10 = cvmae_tree[lranks == 1].min()\n",
        "\n",
        "plt.plot(leaves, np.sqrt(cvmse_tree), label = 'RMSE')\n",
        "plt.plot(leaves, cvmae_tree, label = 'MAE')\n",
        "plt.axvline(log_tree10.best_params_.get('max_leaf_nodes'),\n",
        "            linestyle=\"--\", color=\"black\", label=\"CV estimate\")\n",
        "plt.xlabel(\"Number of leaves\")\n",
        "plt.ylabel(\"Cross-validation RMSE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-pmX4qujv0Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's add these results to our results table\n",
        "model_results = model_results._append({'Model': 'Decision Tree10 - Log',\n",
        "                                       'RMSE': np.sqrt(bestmse_log_tree10),\n",
        "                                       'R2 - MSE': 1-(bestmse_log_tree10/benchMSE),\n",
        "                                       'MAE': bestmae_log_tree10,\n",
        "                                       'R2 - MAE': 1-(bestmae_log_tree10/benchMAE),\n",
        "                                       'p': '-',\n",
        "                                       'p_use': log_tree10.best_params_.get('max_leaf_nodes')},\n",
        "                                       ignore_index=True)\n",
        "\n",
        "model_results"
      ],
      "metadata": {
        "id": "eV2Vwa8Cwi-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tree models work a bit worse than the linear models here.\n",
        "\n",
        "So, why do people like tree models?\n",
        "\n",
        "1. They are simple and fast. They don't require much thought. Scale and monotone transformations of the features don't matter.\n",
        "2. They underlie more complex procedures that we'll talk about next.\n",
        "3. They don't require pre-specifying all the things we think might matter.\n",
        "\n",
        "Finally, remember you should try out linear models as one of your prediction rules in many cases - certainly with small numbers of observations and standard numeric data. (Relatively simple) Linear models will often work well in that setting."
      ],
      "metadata": {
        "id": "33CotEjssn8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Last thing - What about our categorical variables?\n",
        "\n"
      ],
      "metadata": {
        "id": "xce_kLuIx9pb"
      }
    }
  ]
}